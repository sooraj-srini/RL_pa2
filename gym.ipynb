{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQVeqyyI4m-R"
      },
      "source": [
        "# Necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUn5LPyE4va0",
        "outputId": "4016a1b1-709c-44a3-e2db-3e3e52af897b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/953.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m890.9/953.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HkVpaZBd4m-U"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "from torch.functional import F\n",
        "import datetime\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FDfRwjg4m-W"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wEsc6EsT4m-X"
      },
      "outputs": [],
      "source": [
        "LR = 0.001\n",
        "BUFFER_SIZE = 100_000\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "UPDATE_LAG = 20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31puezmP4m-Y"
      },
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "9eXwoEr_4m-Z"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbK7FC7W4m-a"
      },
      "source": [
        "# Dueling DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "T_ltWSYO4m-a"
      },
      "outputs": [],
      "source": [
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, type: int):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc_val = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        self.fc_adv = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_size)\n",
        "        )\n",
        "\n",
        "        self.type = type\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        val = self.fc_val(x)\n",
        "        adv = self.fc_adv(x)\n",
        "\n",
        "        if self.type == 1:\n",
        "            return val + adv - adv.mean()\n",
        "        else:\n",
        "            return val + adv - adv.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "gTDznhy94m-c"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.local = DuelingDQN(state_size, action_size, seed, 1).to(device)\n",
        "        self.target = DuelingDQN(state_size, action_size, seed, 1).to(device)\n",
        "        self.optimizer = optim.Adam(self.local.parameters(), lr=LR)\n",
        "\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "        self.t_step = 0\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        if len(self.memory) >= BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "        self.t_step +=  1\n",
        "        if self.t_step % UPDATE_LAG == 0:\n",
        "            self.update_target()\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target.load_state_dict(self.local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.local(state)\n",
        "        self.local.train()\n",
        "\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ''' Get max predicted Q values (for next states) from target model'''\n",
        "        Q_targets_next = self.target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        ''' Compute Q targets for current states '''\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        ''' Get expected Q values from local model '''\n",
        "        Q_expected = self.local(states).gather(1, actions)\n",
        "\n",
        "        ''' Compute loss '''\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        ''' Minimize the loss '''\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        ''' Gradiant Clipping '''\n",
        "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "        for param in self.local.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Reinforce(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        super(Reinforce, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_size)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return F.softmax(self.fc(x), dim=1)\n",
        "    \n",
        "    def get_action(self, state: torch.Tensor):\n",
        "        probs = self(state)\n",
        "        probs = probs.squeeze(0)\n",
        "        action = probs.multinomial(1)\n",
        "        return action.data[0].item()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRe-9y1I4m-e"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkIIAs0m4m-f",
        "outputId": "e0eca6b6-9587-4dab-9470-16c06120765c"
      },
      "outputs": [],
      "source": [
        "# train the dueling dqn\n",
        "\n",
        "\n",
        "# state_size = env.observation_space.shape[0]\n",
        "# action_size = env.action_space.n\n",
        "\n",
        "def dqn(env, agent, n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "    scores_window = deque(maxlen=100)\n",
        "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "    episode_score = []\n",
        "\n",
        "    eps = eps_start\n",
        "    ''' initialize epsilon '''\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            # print(state)\n",
        "            action = agent.act(state, eps)\n",
        "            # print(action)\n",
        "            something = env.step(action)\n",
        "            # print(something)\n",
        "            next_state, reward, done, _trunc,  _ = something\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "\n",
        "        scores_window.append(score)\n",
        "        episode_score.append(score)\n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "        ''' decrease epsilon '''\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=-100.0:\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "    return episode_score\n",
        "\n",
        "# begin_time = datetime.datetime.now()\n",
        "\n",
        "# agent = Agent(state_size=state_size,action_size = action_size,seed = 0)\n",
        "# dqn(env, agent)\n",
        "\n",
        "# time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "# print(time_taken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reinforce(env, model, optimizer, n_episodes = 10000, max_t = 1000):\n",
        "\n",
        "    scores_window = deque(maxlen=100)\n",
        "    for i_episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            # print(state)\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "            action = model.get_action(state)\n",
        "            # print(action)\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            next_state, reward, done, _trunc,  _ = env.step(action)\n",
        "            state = next_state\n",
        "            # print(reward)\n",
        "            rewards.append(reward)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "        print('\\rEpisode {}\\tCurrent Score: {}\\tAverage Score: {:.2f}'.format(i_episode, score, np.mean(scores_window)), end=\"\")\n",
        "        \n",
        "        G = (torch.Tensor([0]))\n",
        "        for s, a, r in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
        "            G = (torch.Tensor([r])) + 0.99*G\n",
        "            # G.requires_grad = True\n",
        "            # print(G.grad)\n",
        "            # print(model(s)[0])\n",
        "            # s = Variable(s)\n",
        "            prob = model(s)[0][a].squeeze()\n",
        "            # print(prob.requires_grad)\n",
        "            loss = -torch.log(prob) * G\n",
        "            if loss < -100:\n",
        "                break\n",
        "            # print(loss, prob)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # print(\"One round done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"Acrobot-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 23\tCurrent Score: -1000.0\tAverage Score: -996.96"
          ]
        }
      ],
      "source": [
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "model = Reinforce(state_size=state_size, action_size=action_size, seed=42)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "begin_time = datetime.datetime.now()\n",
        "reinforce(env, model, optimizer)\n",
        "time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "print(time_taken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
