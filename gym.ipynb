{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQVeqyyI4m-R"
      },
      "source": [
        "# Necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUn5LPyE4va0",
        "outputId": "132ecf6c-2dc6-439f-c6ab-4251555574f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting torch\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib\n",
            "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.3.0 in /root/.local/lib/python3.10/site-packages (from gymnasium) (4.11.0)\n",
            "Collecting cloudpickle>=1.2.0\n",
            "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting farama-notifications>=0.0.1\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.13.3-py3-none-any.whl (11 kB)\n",
            "Collecting nvidia-nvjitlink-cu12\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /root/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting pillow>=8\n",
            "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /root/.local/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
            "Collecting kiwisolver>=1.3.1\n",
            "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting mpmath>=0.19\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, farama-notifications, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, fsspec, fonttools, filelock, cycler, cloudpickle, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, gymnasium, contourpy, nvidia-cusolver-cu12, matplotlib, torch\n",
            "Successfully installed MarkupSafe-2.1.5 cloudpickle-3.0.0 contourpy-1.2.1 cycler-0.12.1 farama-notifications-0.0.4 filelock-3.13.3 fonttools-4.51.0 fsspec-2024.3.1 gymnasium-0.29.1 jinja2-3.1.3 kiwisolver-1.4.5 matplotlib-3.8.4 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pillow-10.3.0 sympy-1.12 torch-2.2.2 triton-2.2.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HkVpaZBd4m-U"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "from torch.functional import F\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FDfRwjg4m-W"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wEsc6EsT4m-X"
      },
      "outputs": [],
      "source": [
        "LR = 0.001\n",
        "BUFFER_SIZE = 100_000\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "UPDATE_LAG = 20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31puezmP4m-Y"
      },
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9eXwoEr_4m-Z"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbK7FC7W4m-a"
      },
      "source": [
        "# Dueling DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T_ltWSYO4m-a"
      },
      "outputs": [],
      "source": [
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, num_layers, num_nodes, type: int):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        ss = state_size\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc_val = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.fc_val.append(nn.Linear(state_size, num_nodes))\n",
        "            self.fc_val.append(nn.ReLU())\n",
        "            state_size = num_nodes\n",
        "        self.fc_val.append(nn.Linear(num_nodes, 1))\n",
        "        self.fc_val = nn.Sequential(*self.fc_val)        \n",
        "\n",
        "        state_size = ss\n",
        "\n",
        "        self.fc_adv = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.fc_adv.append(nn.Linear(state_size, num_nodes))\n",
        "            self.fc_adv.append(nn.ReLU())\n",
        "            state_size = num_nodes\n",
        "        self.fc_adv.append(nn.Linear(num_nodes, action_size))\n",
        "        self.fc_adv = nn.Sequential(*self.fc_adv)        \n",
        "\n",
        "        self.type = type\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        val = self.fc_val(x)\n",
        "        adv = self.fc_adv(x)\n",
        "\n",
        "        if self.type == 1:\n",
        "            return val + adv - adv.mean()\n",
        "        else:\n",
        "            return val + adv - adv.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gTDznhy94m-c"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, state_size, action_size, num_layers, num_nodes, seed, type, lr = LR):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.local = DuelingDQN(state_size, action_size, seed, num_layers, num_nodes, type).to(device)\n",
        "        self.target = DuelingDQN(state_size, action_size, seed, num_layers, num_nodes, type).to(device)\n",
        "        self.optimizer = optim.Adam(self.local.parameters(), lr=lr)\n",
        "\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        self.t_step = 0\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        if len(self.memory) >= BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "        self.t_step +=  1\n",
        "        if self.t_step % UPDATE_LAG == 0:\n",
        "            self.update_target()\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target.load_state_dict(self.local.state_dict())\n",
        "\n",
        "    def act(self, state, policy, eps=0., tau=0.):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.local(state)\n",
        "        self.local.train()\n",
        "\n",
        "        if policy == 'eps':\n",
        "            if random.random() > eps:\n",
        "                return np.argmax(action_values.cpu().data.numpy())\n",
        "            else:\n",
        "                return random.choice(np.arange(self.action_size))\n",
        "        else:\n",
        "            act = action_values.cpu().data.numpy()\n",
        "            act = act - np.max(act)\n",
        "            exp = np.exp(act / tau)\n",
        "            probs = exp / np.sum(exp)\n",
        "            return np.random.choice(np.arange(self.action_size), p=probs.squeeze())\n",
        "\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ''' Get max predicted Q values (for next states) from target model'''\n",
        "        Q_targets_next = self.target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        ''' Compute Q targets for current states '''\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        ''' Get expected Q values from local model '''\n",
        "        Q_expected = self.local(states).gather(1, actions)\n",
        "\n",
        "        ''' Compute loss '''\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        ''' Minimize the loss '''\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        ''' Gradiant Clipping '''\n",
        "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "        for param in self.local.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjGjyIKH3efq"
      },
      "source": [
        "# REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tvtQZMF43efq"
      },
      "outputs": [],
      "source": [
        "class Reinforce(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, num_layers, num_nodes):\n",
        "        super(Reinforce, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.fc.append(nn.Linear(state_size, num_nodes))\n",
        "            self.fc.append(nn.ReLU())\n",
        "            state_size = num_nodes\n",
        "        self.fc.append(nn.Linear(num_nodes, action_size))\n",
        "        self.fc = nn.Sequential(*self.fc)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return F.softmax(self.fc(x), dim=1)\n",
        "\n",
        "    def get_action(self, state: torch.Tensor):\n",
        "        # print(state)\n",
        "        probs = self(state)\n",
        "        probs = probs.squeeze(0)\n",
        "        model = torch.distributions.Categorical(probs)\n",
        "        action = model.sample()\n",
        "        return action.item(), model.log_prob(action)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        super(Critic, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.value_fc = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor):\n",
        "      return self.value_fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRe-9y1I4m-e"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MkIIAs0m4m-f"
      },
      "outputs": [],
      "source": [
        "# train the dueling dqn\n",
        "\n",
        "\n",
        "# state_size = env.observation_space.shape[0]\n",
        "# action_size = env.action_space.n\n",
        "\n",
        "def dqn(env, agent, policy, n_episodes=10000, max_t=1000, policy_val = 1.0, decay = 1.0, thresh = -100):\n",
        "    scores_window = deque(maxlen=100)\n",
        "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "    episode_score = []\n",
        "\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            # print(state)\n",
        "            action = None\n",
        "            action = agent.act(state, policy, eps=policy_val, tau=policy_val)\n",
        "            # print(action)\n",
        "            something = env.step(action)\n",
        "            # print(something)\n",
        "            next_state, reward, done, _trunc,  _ = something\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "\n",
        "        scores_window.append(score)\n",
        "        episode_score.append(score)\n",
        "        policy_val = max(decay*policy_val, 0.05)\n",
        "\n",
        "\n",
        "        print('\\rEpisode {}\\tCurrent Score: {:.2f}\\tAverage Score: {:.2f}'.format(i_episode, score, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=thresh:\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "    return episode_score\n",
        "\n",
        "# begin_time = datetime.datetime.now()\n",
        "\n",
        "# agent = Agent(state_size=state_size,action_size = action_size,seed = 0)\n",
        "# dqn(env, agent)\n",
        "\n",
        "# time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "# print(time_taken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "sNcJGb0G3efs"
      },
      "outputs": [],
      "source": [
        "def reinforce(env, model, optimizer, n_episodes = 1000, max_t = 2000, baseline = False, critic = None, critic_optimizer = None, thresh = -100):\n",
        "\n",
        "    episode_scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    for i_episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        score = 0\n",
        "        log_probs = []\n",
        "        for t in range(max_t):\n",
        "            # print(state)\n",
        "            # state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "            action, lp = model.get_action(state)\n",
        "            # print(action)\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            log_probs.append(lp)\n",
        "            next_state, reward, done, _trunc,  _ = env.step(action)\n",
        "            # print(f\"\\rreward: {reward}\", end=\"\")\n",
        "            state = next_state\n",
        "            # print(reward)\n",
        "            rewards.append(reward)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "        # break\n",
        "        scores_window.append(score)\n",
        "        episode_scores.append(score)\n",
        "        print('\\rEpisode {}\\tCurrent Score: {}\\tAverage Score: {:.2f}'.format(i_episode, score, np.mean(scores_window)), end=\"\")\n",
        "        if np.mean(scores_window) >= thresh:\n",
        "            print(f\"\\nEpisode completed in {i_episode} episodes.\")\n",
        "            break\n",
        "\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for s, a, r in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
        "            G = r + 0.99*G\n",
        "\n",
        "            returns.append(G)\n",
        "            # G.requires_grad = True\n",
        "            # print(G.grad)\n",
        "            # print(model(s)[0])\n",
        "            # s = Variable(s)\n",
        "\n",
        "        returns = torch.tensor(returns[::-1])\n",
        "        # returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "        losses = []\n",
        "        value_losses = []\n",
        "        for s, a, g, r, lp in zip(states, actions, returns, rewards, log_probs):\n",
        "            # prob, val = model(s)\n",
        "            # prob = model(s)[0][a].squeeze()\n",
        "            val = 0\n",
        "            if baseline:\n",
        "              val = critic(s)\n",
        "              # print(val[0].squeeze())\n",
        "              td_error = (g - val[0].squeeze())\n",
        "              value_losses.append(0.5 * (td_error ** 2))\n",
        "            # print(prob.requires_grad)\n",
        "            # print(lp)\n",
        "            loss = -lp * (g - val)\n",
        "            # print(lp, g)\n",
        "            # print(loss)\n",
        "            losses.append(loss)\n",
        "\n",
        "            # print(loss, G)\n",
        "            # if loss < -100:\n",
        "            #     print(loss)\n",
        "            # # print(loss, prob)\n",
        "            # optimizer.zero_grad()\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "        # for i in range(len(losses)//32 - 1):\n",
        "           \n",
        "        # loss = torch.stack(losses[i*32:(i+1)*32]).sum()\n",
        "        loss = torch.stack(losses).sum()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        # print(f\"\\rloss:{torch.stack(losses).sum()}\")\n",
        "        # print(f\"\\rloss:{torch.sum(torch.stack(losses))}\")\n",
        "        # print(torch.sum(torch.stack(losses)))\n",
        "        \n",
        "        # for param in model.parameters():\n",
        "        #     param.grad.data.clamp_(-1, 1)\n",
        "            # print(param.grad.data)\n",
        "        optimizer.step()\n",
        "        # if baseline:\n",
        "        #   value_loss.backward()\n",
        "\n",
        "        if baseline:\n",
        "          value_loss = torch.stack(value_losses).sum()\n",
        "          # print(value_loss)\n",
        "          critic_optimizer.zero_grad()\n",
        "          value_loss.backward()\n",
        "        #   for param in critic.parameters():\n",
        "        #     param.grad.data.clamp_(-1, 1)\n",
        "          critic_optimizer.step()\n",
        "        # break\n",
        "\n",
        "        # print(\"One round done\")\n",
        "    return episode_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdkBesA93efs"
      },
      "source": [
        "# Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4NWWkImp3eft"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Acrobot-v1\")\n",
        "env.action_space.seed(0)\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_layers = (2, 3, 4)\n",
        "num_nodes = (64, 128)\n",
        "eps_vals = (0.3, 0.2, 0.1)\n",
        "tau_vals = (0.5, 0.3, 0.1)\n",
        "params = [(layers , n, 'eps', e) for layers in num_layers for n in num_nodes for e in eps_vals]\n",
        "params += [(layers , n, 'soft', t) for layers in num_layers for n in num_nodes for t in tau_vals]\n",
        "best_params = None\n",
        "least_regret = -int(1e9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize DDQN 1 for Acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 64 eps 0.3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: -121.31Average Score: -121.31\n",
            "Episode 168\tCurrent Score: -98.00\tAverage Score: -99.9651\n",
            "Environment solved in 168 episodes!\tAverage Score: -99.96\n",
            "0:00:40.806991\n",
            "-112.80357142857143\n",
            "2 64 eps 0.2\n",
            "Episode 100\tAverage Score: -105.45Average Score: -105.45\n",
            "Episode 107\tCurrent Score: -95.00\tAverage Score: -99.2835\n",
            "Environment solved in 107 episodes!\tAverage Score: -99.28\n",
            "0:00:23.863075\n",
            "-105.07476635514018\n",
            "2 64 eps 0.1\n",
            "Episode 100\tAverage Score: -107.69Average Score: -107.69\n",
            "Episode 112\tCurrent Score: -71.00\tAverage Score: -99.9422\n",
            "Environment solved in 112 episodes!\tAverage Score: -99.94\n",
            "0:00:25.795769\n",
            "-106.47321428571429\n",
            "2 128 eps 0.3\n",
            "Episode 100\tAverage Score: -119.26\tAverage Score: -119.26\n",
            "Episode 132\tCurrent Score: -98.00\tAverage Score: -100.007\n",
            "Environment solved in 132 episodes!\tAverage Score: -100.00\n",
            "0:00:34.187453\n",
            "-113.40151515151516\n",
            "2 128 eps 0.2\n",
            "Episode 100\tAverage Score: -110.34\tAverage Score: -110.34\n",
            "Episode 113\tCurrent Score: -89.00\tAverage Score: -99.7214\n",
            "Environment solved in 113 episodes!\tAverage Score: -99.72\n",
            "0:00:26.791683\n",
            "-108.49557522123894\n",
            "2 128 eps 0.1\n",
            "Episode 100\tAverage Score: -119.22\tAverage Score: -119.22\n",
            "Episode 193\tCurrent Score: -101.00\tAverage Score: -93.786\n",
            "Environment solved in 193 episodes!\tAverage Score: -93.78\n",
            "0:00:45.052401\n",
            "-106.74611398963731\n",
            "3 64 eps 0.3\n",
            "Episode 100\tAverage Score: -127.28Average Score: -127.28\n",
            "Episode 149\tCurrent Score: -89.00\tAverage Score: -99.7758\n",
            "Environment solved in 149 episodes!\tAverage Score: -99.77\n",
            "0:00:44.527239\n",
            "-115.91946308724832\n",
            "3 64 eps 0.2\n",
            "Episode 100\tAverage Score: -108.63Average Score: -108.63\n",
            "Episode 102\tCurrent Score: -94.00\tAverage Score: -99.939\n",
            "Environment solved in 102 episodes!\tAverage Score: -99.93\n",
            "0:00:27.360249\n",
            "-108.02941176470588\n",
            "3 64 eps 0.1\n",
            "Episode 100\tAverage Score: -109.77Average Score: -109.77\n",
            "Episode 104\tCurrent Score: -91.00\tAverage Score: -99.682\n",
            "Environment solved in 104 episodes!\tAverage Score: -99.68\n",
            "0:00:29.422714\n",
            "-109.07692307692308\n",
            "3 128 eps 0.3\n",
            "Episode 100\tAverage Score: -123.81Average Score: -123.814\n",
            "Episode 154\tCurrent Score: -120.00\tAverage Score: -95.794\n",
            "Environment solved in 154 episodes!\tAverage Score: -95.79\n",
            "0:00:45.313525\n",
            "-112.77922077922078\n",
            "3 128 eps 0.2\n",
            "Episode 100\tAverage Score: -122.42\tAverage Score: -122.42\n",
            "Episode 139\tCurrent Score: -97.00\tAverage Score: -99.5464\n",
            "Environment solved in 139 episodes!\tAverage Score: -99.54\n",
            "0:00:41.164955\n",
            "-113.83453237410072\n",
            "3 128 eps 0.1\n",
            "Episode 100\tAverage Score: -109.07Average Score: -109.07\n",
            "Episode 103\tCurrent Score: -94.00\tAverage Score: -99.2681\n",
            "Environment solved in 103 episodes!\tAverage Score: -99.26\n",
            "0:00:28.957885\n",
            "-108.55339805825243\n",
            "4 64 eps 0.3\n",
            "Episode 100\tAverage Score: -117.95Average Score: -117.95\n",
            "Episode 135\tCurrent Score: -98.00\tAverage Score: -99.8630\n",
            "Environment solved in 135 episodes!\tAverage Score: -99.86\n",
            "0:00:43.004993\n",
            "-111.43703703703704\n",
            "4 64 eps 0.2\n",
            "Episode 100\tAverage Score: -131.09Average Score: -131.09\n",
            "Episode 133\tCurrent Score: -85.00\tAverage Score: -99.9910\n",
            "Environment solved in 133 episodes!\tAverage Score: -99.99\n",
            "0:00:46.755295\n",
            "-121.10526315789474\n",
            "4 64 eps 0.1\n",
            "Episode 100\tAverage Score: -112.95Average Score: -112.95\n",
            "Episode 108\tCurrent Score: -69.00\tAverage Score: -98.7921\n",
            "Environment solved in 108 episodes!\tAverage Score: -98.79\n",
            "0:00:35.696904\n",
            "-110.75\n",
            "4 128 eps 0.3\n",
            "Episode 100\tAverage Score: -130.90Average Score: -130.90\n",
            "Episode 192\tCurrent Score: -82.00\tAverage Score: -99.9334\n",
            "Environment solved in 192 episodes!\tAverage Score: -99.93\n",
            "0:01:07.033511\n",
            "-114.875\n",
            "4 128 eps 0.2\n",
            "Episode 100\tAverage Score: -123.86\tAverage Score: -123.86\n",
            "Episode 160\tCurrent Score: -78.00\tAverage Score: -99.7982\n",
            "Environment solved in 160 episodes!\tAverage Score: -99.79\n",
            "0:00:53.184915\n",
            "-111.225\n",
            "4 128 eps 0.1\n",
            "Episode 100\tAverage Score: -116.03\tAverage Score: -116.03\n",
            "Episode 112\tCurrent Score: -72.00\tAverage Score: -99.5317\n",
            "Environment solved in 112 episodes!\tAverage Score: -99.53\n",
            "0:00:37.949091\n",
            "-113.4375\n",
            "2 64 soft 0.5\n",
            "Episode 100\tAverage Score: -122.09Average Score: -122.094\n",
            "Episode 145\tCurrent Score: -102.00\tAverage Score: -94.296\n",
            "Environment solved in 145 episodes!\tAverage Score: -94.29\n",
            "0:00:38.059817\n",
            "-113.64137931034483\n",
            "2 64 soft 0.3\n",
            "Episode 100\tAverage Score: -105.79Average Score: -105.79\n",
            "Episode 114\tCurrent Score: -82.00\tAverage Score: -99.8010\n",
            "Environment solved in 114 episodes!\tAverage Score: -99.80\n",
            "0:00:27.391060\n",
            "-104.93859649122807\n",
            "2 64 soft 0.1\n",
            "Episode 100\tAverage Score: -115.56Average Score: -115.56\n",
            "Episode 129\tCurrent Score: -73.00\tAverage Score: -99.9936\n",
            "Environment solved in 129 episodes!\tAverage Score: -99.99\n",
            "0:00:33.703716\n",
            "-113.54263565891473\n",
            "2 128 soft 0.5\n",
            "Episode 100\tAverage Score: -122.66Average Score: -122.66\n",
            "Episode 130\tCurrent Score: -74.00\tAverage Score: -99.9348\n",
            "Environment solved in 130 episodes!\tAverage Score: -99.93\n",
            "0:00:33.773642\n",
            "-115.20769230769231\n",
            "2 128 soft 0.3\n",
            "Episode 100\tAverage Score: -104.54Average Score: -104.54\n",
            "Episode 103\tCurrent Score: -74.00\tAverage Score: -99.583\n",
            "Environment solved in 103 episodes!\tAverage Score: -99.58\n",
            "0:00:24.290414\n",
            "-103.58252427184466\n",
            "2 128 soft 0.1\n",
            "Episode 100\tAverage Score: -108.22Average Score: -108.22\n",
            "Episode 102\tCurrent Score: -72.00\tAverage Score: -96.180\n",
            "Environment solved in 102 episodes!\tAverage Score: -96.18\n",
            "0:00:25.243985\n",
            "-107.70588235294117\n",
            "3 64 soft 0.5\n",
            "Episode 100\tAverage Score: -126.42Average Score: -126.420\n",
            "Episode 108\tCurrent Score: -86.00\tAverage Score: -99.1324\n",
            "Environment solved in 108 episodes!\tAverage Score: -99.13\n",
            "0:00:34.213373\n",
            "-124.61111111111111\n",
            "3 64 soft 0.3\n",
            "Episode 100\tAverage Score: -108.36Average Score: -108.36\n",
            "Episode 103\tCurrent Score: -95.00\tAverage Score: -99.9878\n",
            "Environment solved in 103 episodes!\tAverage Score: -99.98\n",
            "0:00:29.393504\n",
            "-108.41747572815534\n",
            "3 64 soft 0.1\n",
            "Episode 100\tAverage Score: -112.00Average Score: -112.00\n",
            "Episode 103\tCurrent Score: -108.00\tAverage Score: -99.89\n",
            "Environment solved in 103 episodes!\tAverage Score: -99.89\n",
            "0:00:30.306154\n",
            "-111.41747572815534\n",
            "3 128 soft 0.5\n",
            "Episode 100\tAverage Score: -116.57Average Score: -116.57\n",
            "Episode 125\tCurrent Score: -70.00\tAverage Score: -99.9682\n",
            "Environment solved in 125 episodes!\tAverage Score: -99.96\n",
            "0:00:37.374884\n",
            "-111.184\n",
            "3 128 soft 0.3\n",
            "Episode 100\tAverage Score: -107.41Average Score: -107.41\n",
            "Episode 102\tCurrent Score: -87.00\tAverage Score: -98.220\n",
            "Environment solved in 102 episodes!\tAverage Score: -98.22\n",
            "0:00:30.021052\n",
            "-107.12745098039215\n",
            "3 128 soft 0.1\n",
            "Episode 100\tAverage Score: -123.12Average Score: -123.120\n",
            "Episode 116\tCurrent Score: -76.00\tAverage Score: -92.6357\n",
            "Environment solved in 116 episodes!\tAverage Score: -92.63\n",
            "0:00:36.272097\n",
            "-117.65517241379311\n",
            "4 64 soft 0.5\n",
            "Episode 100\tAverage Score: -129.95Average Score: -129.95\n",
            "Episode 139\tCurrent Score: -108.00\tAverage Score: -99.590\n",
            "Environment solved in 139 episodes!\tAverage Score: -99.59\n",
            "0:00:48.972712\n",
            "-119.53956834532374\n",
            "4 64 soft 0.3\n",
            "Episode 100\tAverage Score: -116.77Average Score: -116.770\n",
            "Episode 104\tCurrent Score: -78.00\tAverage Score: -99.5713\n",
            "Environment solved in 104 episodes!\tAverage Score: -99.57\n",
            "0:00:35.374795\n",
            "-115.6826923076923\n",
            "4 64 soft 0.1\n",
            "Episode 100\tAverage Score: -113.35Average Score: -113.35\n",
            "Episode 119\tCurrent Score: -62.00\tAverage Score: -99.9253\n",
            "Environment solved in 119 episodes!\tAverage Score: -99.92\n",
            "0:00:38.112217\n",
            "-109.16806722689076\n",
            "4 128 soft 0.5\n",
            "Episode 100\tAverage Score: -163.50\tAverage Score: -163.50\n",
            "Episode 148\tCurrent Score: -93.00\tAverage Score: -95.5218\n",
            "Environment solved in 148 episodes!\tAverage Score: -95.52\n",
            "0:01:03.380286\n",
            "-139.72297297297297\n",
            "4 128 soft 0.3\n",
            "Episode 100\tAverage Score: -201.50Average Score: -201.507\n",
            "Episode 146\tCurrent Score: -107.00\tAverage Score: -99.916\n",
            "Environment solved in 146 episodes!\tAverage Score: -99.91\n",
            "0:01:14.413210\n",
            "-167.94520547945206\n",
            "4 128 soft 0.1\n",
            "Episode 100\tAverage Score: -146.98Average Score: -146.983\n",
            "Episode 156\tCurrent Score: -86.00\tAverage Score: -99.7323\n",
            "Environment solved in 156 episodes!\tAverage Score: -99.73\n",
            "0:00:59.931919\n",
            "-127.19871794871794\n",
            "(2, 128, 'soft', 0.3)\n"
          ]
        }
      ],
      "source": [
        "for layer, n, policy, val in params:\n",
        "    print(layer, n, policy, val)\n",
        "    begin_time = datetime.datetime.now()\n",
        "    agent = Agent(state_size=state_size,action_size = action_size,seed = 0, num_layers=layer, num_nodes=n, type=1)\n",
        "    scores = dqn(env, agent, policy, policy_val=val)\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "    if least_regret < sum(scores)/len(scores):\n",
        "        least_regret = sum(scores)/len(scores)\n",
        "        best_params = (layer, n, policy, val)\n",
        "\n",
        "    print(time_taken)\n",
        "    print(sum(scores)/len(scores))\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize DDQN 2 for Acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 64 eps 0.3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: -128.99Average Score: -128.99\n",
            "Episode 195\tCurrent Score: -84.00\tAverage Score: -99.9862\n",
            "Environment solved in 195 episodes!\tAverage Score: -99.98\n",
            "0:00:50.396191\n",
            "-115.1076923076923\n",
            "2 64 eps 0.2\n",
            "Episode 100\tAverage Score: -113.24Average Score: -113.24\n",
            "Episode 131\tCurrent Score: -74.00\tAverage Score: -99.9559\n",
            "Environment solved in 131 episodes!\tAverage Score: -99.95\n",
            "0:00:30.777650\n",
            "-107.42748091603053\n",
            "2 64 eps 0.1\n",
            "Episode 100\tAverage Score: -122.96Average Score: -122.96\n",
            "Episode 142\tCurrent Score: -88.00\tAverage Score: -99.7348\n",
            "Environment solved in 142 episodes!\tAverage Score: -99.73\n",
            "0:00:36.769243\n",
            "-113.55633802816901\n",
            "2 128 eps 0.3\n",
            "Episode 100\tAverage Score: -125.74\tAverage Score: -125.74\n",
            "Episode 198\tCurrent Score: -91.00\tAverage Score: -99.1065\n",
            "Environment solved in 198 episodes!\tAverage Score: -99.10\n",
            "0:00:51.969632\n",
            "-112.61616161616162\n",
            "2 128 eps 0.2\n",
            "Episode 100\tAverage Score: -124.85\tAverage Score: -124.85\n",
            "Episode 121\tCurrent Score: -78.00\tAverage Score: -99.8655\n",
            "Environment solved in 121 episodes!\tAverage Score: -99.86\n",
            "0:00:33.078394\n",
            "-118.17355371900827\n",
            "2 128 eps 0.1\n",
            "Episode 100\tAverage Score: -119.38Average Score: -119.380\n",
            "Episode 189\tCurrent Score: -107.00\tAverage Score: -96.819\n",
            "Environment solved in 189 episodes!\tAverage Score: -96.81\n",
            "0:00:47.398829\n",
            "-107.76719576719577\n",
            "3 64 eps 0.3\n",
            "Episode 100\tAverage Score: -119.06Average Score: -119.06\n",
            "Episode 132\tCurrent Score: -74.00\tAverage Score: -99.9055\n",
            "Environment solved in 132 episodes!\tAverage Score: -99.90\n",
            "0:00:38.615265\n",
            "-112.09848484848484\n",
            "3 64 eps 0.2\n",
            "Episode 100\tAverage Score: -104.33Average Score: -104.33\n",
            "Episode 104\tCurrent Score: -102.00\tAverage Score: -99.45\n",
            "Environment solved in 104 episodes!\tAverage Score: -99.45\n",
            "0:00:28.943798\n",
            "-103.49038461538461\n",
            "3 64 eps 0.1\n",
            "Episode 100\tAverage Score: -104.69Average Score: -104.69\n",
            "Episode 103\tCurrent Score: -74.00\tAverage Score: -99.996\n",
            "Environment solved in 103 episodes!\tAverage Score: -99.99\n",
            "0:00:28.200116\n",
            "-104.09708737864078\n",
            "3 128 eps 0.3\n",
            "Episode 100\tAverage Score: -117.69Average Score: -117.69\n",
            "Episode 121\tCurrent Score: -89.00\tAverage Score: -99.8466\n",
            "Environment solved in 121 episodes!\tAverage Score: -99.84\n",
            "0:00:36.358098\n",
            "-113.41322314049587\n",
            "3 128 eps 0.2\n",
            "Episode 100\tAverage Score: -113.21Average Score: -113.210\n",
            "Episode 109\tCurrent Score: -81.00\tAverage Score: -99.4157\n",
            "Environment solved in 109 episodes!\tAverage Score: -99.41\n",
            "0:00:32.095083\n",
            "-111.18348623853211\n",
            "3 128 eps 0.1\n",
            "Episode 100\tAverage Score: -104.21Average Score: -104.21\n",
            "Episode 102\tCurrent Score: -83.00\tAverage Score: -98.2531\n",
            "Environment solved in 102 episodes!\tAverage Score: -98.25\n",
            "0:00:28.525052\n",
            "-104.16666666666667\n",
            "4 64 eps 0.3\n",
            "Episode 100\tAverage Score: -131.64Average Score: -131.64\n",
            "Episode 198\tCurrent Score: -104.00\tAverage Score: -94.316\n",
            "Environment solved in 198 episodes!\tAverage Score: -94.31\n",
            "0:01:08.323724\n",
            "-112.98989898989899\n",
            "4 64 eps 0.2\n",
            "Episode 100\tAverage Score: -116.73Average Score: -116.73\n",
            "Episode 112\tCurrent Score: -73.00\tAverage Score: -99.6078\n",
            "Environment solved in 112 episodes!\tAverage Score: -99.60\n",
            "0:00:37.921818\n",
            "-113.46428571428571\n",
            "4 64 eps 0.1\n",
            "Episode 100\tAverage Score: -108.61Average Score: -108.61\n",
            "Episode 102\tCurrent Score: -83.00\tAverage Score: -95.9815\n",
            "Environment solved in 102 episodes!\tAverage Score: -95.98\n",
            "0:00:32.634159\n",
            "-110.12745098039215\n",
            "4 128 eps 0.3\n",
            "Episode 100\tAverage Score: -122.71Average Score: -122.71\n",
            "Episode 164\tCurrent Score: -94.00\tAverage Score: -96.2809\n",
            "Environment solved in 164 episodes!\tAverage Score: -96.28\n",
            "0:00:53.992730\n",
            "-110.5\n",
            "4 128 eps 0.2\n",
            "Episode 100\tAverage Score: -134.07\tAverage Score: -134.07\n",
            "Episode 136\tCurrent Score: -78.00\tAverage Score: -99.5858\n",
            "Environment solved in 136 episodes!\tAverage Score: -99.58\n",
            "0:00:50.330357\n",
            "-123.53676470588235\n",
            "4 128 eps 0.1\n",
            "Episode 100\tAverage Score: -116.45Average Score: -116.45\n",
            "Episode 129\tCurrent Score: -85.00\tAverage Score: -99.9818\n",
            "Environment solved in 129 episodes!\tAverage Score: -99.98\n",
            "0:00:43.173063\n",
            "-109.3875968992248\n",
            "2 64 soft 0.5\n",
            "Episode 100\tAverage Score: -134.88Average Score: -134.88\n",
            "Episode 180\tCurrent Score: -77.00\tAverage Score: -99.71818\n",
            "Environment solved in 180 episodes!\tAverage Score: -99.71\n",
            "0:00:50.378177\n",
            "-120.24444444444444\n",
            "2 64 soft 0.3\n",
            "Episode 100\tAverage Score: -120.90Average Score: -120.90\n",
            "Episode 151\tCurrent Score: -103.00\tAverage Score: -99.244\n",
            "Environment solved in 151 episodes!\tAverage Score: -99.24\n",
            "0:00:37.901812\n",
            "-109.74834437086092\n",
            "2 64 soft 0.1\n",
            "Episode 100\tAverage Score: -118.32\tAverage Score: -118.32\n",
            "Episode 116\tCurrent Score: -81.00\tAverage Score: -94.2983\n",
            "Environment solved in 116 episodes!\tAverage Score: -94.29\n",
            "0:00:30.161201\n",
            "-115.87931034482759\n",
            "2 128 soft 0.5\n",
            "Episode 100\tAverage Score: -138.99Average Score: -138.997\n",
            "Episode 182\tCurrent Score: -87.00\tAverage Score: -93.2697\n",
            "Environment solved in 182 episodes!\tAverage Score: -93.26\n",
            "0:00:50.376235\n",
            "-116.96153846153847\n",
            "2 128 soft 0.3\n",
            "Episode 100\tAverage Score: -124.50Average Score: -124.50\n",
            "Episode 110\tCurrent Score: -79.00\tAverage Score: -99.2256\n",
            "Environment solved in 110 episodes!\tAverage Score: -99.22\n",
            "0:00:31.875454\n",
            "-121.10909090909091\n",
            "2 128 soft 0.1\n",
            "Episode 47\tCurrent Score: -78.00\tAverage Score: -118.302"
          ]
        }
      ],
      "source": [
        "least_regret = -int(1e9)\n",
        "for layer, n, policy, val in params:\n",
        "    print(layer, n, policy, val)\n",
        "    begin_time = datetime.datetime.now()\n",
        "    agent = Agent(state_size=state_size,action_size = action_size,seed = 0, num_layers=layer, num_nodes=n, type=2)\n",
        "    scores = dqn(env, agent, policy, policy_val=val)\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "    if least_regret < sum(scores)/len(scores):\n",
        "        least_regret = sum(scores)/len(scores)\n",
        "        best_params = (layer, n, policy, val)\n",
        "    print(time_taken)\n",
        "    print(sum(scores)/len(scores))\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 64 soft 0.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: -126.51Average Score: -126.511\n",
            "Episode 169\tCurrent Score: -88.00\tAverage Score: -91.8801\n",
            "Environment solved in 169 episodes!\tAverage Score: -91.88\n",
            "0:00:43.230890\n",
            "-112.9585798816568\n",
            "2 64 soft 0.3\n",
            "Episode 100\tAverage Score: -115.27Average Score: -115.27\n",
            "Episode 110\tCurrent Score: -75.00\tAverage Score: -99.3550\n",
            "Environment solved in 110 episodes!\tAverage Score: -99.35\n",
            "0:00:28.221013\n",
            "-112.71818181818182\n",
            "2 64 soft 0.1\n",
            "Episode 100\tAverage Score: -120.97Average Score: -120.972\n",
            "Episode 150\tCurrent Score: -70.00\tAverage Score: -96.5080\n",
            "Environment solved in 150 episodes!\tAverage Score: -96.50\n",
            "0:00:38.246415\n",
            "-111.64\n",
            "2 128 soft 0.5\n",
            "Episode 100\tAverage Score: -110.11\tAverage Score: -110.11\n",
            "Episode 108\tCurrent Score: -60.00\tAverage Score: -99.1613\n",
            "Environment solved in 108 episodes!\tAverage Score: -99.16\n",
            "0:00:27.536486\n",
            "-109.21296296296296\n",
            "2 128 soft 0.3\n",
            "Episode 100\tAverage Score: -124.82Average Score: -124.82\n",
            "Episode 151\tCurrent Score: -80.00\tAverage Score: -98.7433\n",
            "Environment solved in 151 episodes!\tAverage Score: -98.74\n",
            "0:00:40.329162\n",
            "-114.94701986754967\n",
            "2 128 soft 0.1\n",
            "Episode 100\tAverage Score: -108.40\tAverage Score: -108.40\n",
            "Episode 106\tCurrent Score: -124.00\tAverage Score: -99.814\n",
            "Environment solved in 106 episodes!\tAverage Score: -99.81\n",
            "0:00:27.179675\n",
            "-108.5377358490566\n",
            "3 64 soft 0.5\n",
            "Episode 100\tAverage Score: -132.50Average Score: -132.500\n",
            "Episode 134\tCurrent Score: -69.00\tAverage Score: -99.9134\n",
            "Environment solved in 134 episodes!\tAverage Score: -99.91\n",
            "0:00:43.764757\n",
            "-120.85820895522389\n",
            "3 64 soft 0.3\n",
            "Episode 100\tAverage Score: -110.36\tAverage Score: -110.36\n",
            "Episode 105\tCurrent Score: -74.00\tAverage Score: -99.5504\n",
            "Environment solved in 105 episodes!\tAverage Score: -99.55\n",
            "0:00:29.893309\n",
            "-109.39047619047619\n",
            "3 64 soft 0.1\n",
            "Episode 100\tAverage Score: -100.60Average Score: -100.60\n",
            "Episode 101\tCurrent Score: -70.00\tAverage Score: -98.60\n",
            "Environment solved in 101 episodes!\tAverage Score: -98.60\n",
            "0:00:27.498867\n",
            "-100.29702970297029\n",
            "3 128 soft 0.5\n",
            "Episode 100\tAverage Score: -119.97Average Score: -119.97\n",
            "Episode 115\tCurrent Score: -78.00\tAverage Score: -99.8137\n",
            "Environment solved in 115 episodes!\tAverage Score: -99.81\n",
            "0:00:36.439354\n",
            "-116.86086956521739\n",
            "3 128 soft 0.3\n",
            "Episode 100\tAverage Score: -116.41Average Score: -116.41\n",
            "Episode 103\tCurrent Score: -74.00\tAverage Score: -97.295\n",
            "Environment solved in 103 episodes!\tAverage Score: -97.29\n",
            "0:00:32.020192\n",
            "-115.4368932038835\n",
            "3 128 soft 0.1\n",
            "Episode 100\tAverage Score: -102.75Average Score: -102.75\n",
            "Episode 103\tCurrent Score: -84.00\tAverage Score: -99.9529\n",
            "Environment solved in 103 episodes!\tAverage Score: -99.95\n",
            "0:00:29.019915\n",
            "-102.58252427184466\n",
            "4 64 soft 0.5\n",
            "Episode 100\tAverage Score: -135.93Average Score: -135.930\n",
            "Episode 110\tCurrent Score: -78.00\tAverage Score: -99.3303\n",
            "Environment solved in 110 episodes!\tAverage Score: -99.33\n",
            "0:00:45.184761\n",
            "-132.24545454545455\n",
            "4 64 soft 0.3\n",
            "Episode 100\tAverage Score: -124.27Average Score: -124.270\n",
            "Episode 140\tCurrent Score: -67.00\tAverage Score: -99.8068\n",
            "Environment solved in 140 episodes!\tAverage Score: -99.80\n",
            "0:00:49.164102\n",
            "-114.87142857142857\n",
            "4 64 soft 0.1\n",
            "Episode 100\tAverage Score: -111.12Average Score: -111.120\n",
            "Episode 102\tCurrent Score: -74.00\tAverage Score: -99.5813\n",
            "Environment solved in 102 episodes!\tAverage Score: -99.58\n",
            "0:00:35.049774\n",
            "-110.65686274509804\n",
            "4 128 soft 0.5\n",
            "Episode 100\tAverage Score: -125.34Average Score: -125.340\n",
            "Episode 111\tCurrent Score: -69.00\tAverage Score: -99.639\n",
            "Environment solved in 111 episodes!\tAverage Score: -99.63\n",
            "0:00:41.129855\n",
            "-120.63063063063063\n",
            "4 128 soft 0.3\n",
            "Episode 100\tAverage Score: -105.27Average Score: -105.27\n",
            "Episode 101\tCurrent Score: -107.00\tAverage Score: -99.49\n",
            "Environment solved in 101 episodes!\tAverage Score: -99.49\n",
            "0:00:32.256516\n",
            "-105.2871287128713\n",
            "4 128 soft 0.1\n",
            "Episode 100\tAverage Score: -112.44\tAverage Score: -112.44\n",
            "Episode 102\tCurrent Score: -71.00\tAverage Score: -98.517\n",
            "Environment solved in 102 episodes!\tAverage Score: -98.51\n",
            "0:00:35.475716\n",
            "-111.84313725490196\n",
            "(3, 64, 'soft', 0.1)\n"
          ]
        }
      ],
      "source": [
        "least_regret = -103.49\n",
        "best_params = (3, 64, 'eps', 0.2)\n",
        "for layer, n, policy, val in params[18:]:\n",
        "    print(layer, n, policy, val)\n",
        "    begin_time = datetime.datetime.now()\n",
        "    agent = Agent(state_size=state_size,action_size = action_size,seed = 0, num_layers=layer, num_nodes=n, type=2)\n",
        "    scores = dqn(env, agent, policy, policy_val=val)\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "    if least_regret < sum(scores)/len(scores):\n",
        "        least_regret = sum(scores)/len(scores)\n",
        "        best_params = (layer, n, policy, val)\n",
        "    print(time_taken)\n",
        "    print(sum(scores)/len(scores))\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize Reinforce 1 for Acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_layers = (2, 3, 4)\n",
        "num_nodes = (64, 128)\n",
        "params = [(layers , n, 'eps', 0) for layers in num_layers for n in num_nodes]\n",
        "# params += [(layers , n, 'soft', t) for layers in num_layers for n in num_nodes for t in tau_vals]\n",
        "best_params = None\n",
        "least_regret = -int(1e9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 128\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 886\tCurrent Score: -91.0\tAverage Score: -99.93347\n",
            "Episode completed in 886 episodes.\n",
            "0:01:41.505891\n",
            "-151.74520856820743\n",
            "3 64\n",
            "Episode 999\tCurrent Score: -279.0\tAverage Score: -138.7280:03:37.903416\n",
            "-266.832\n",
            "3 128\n",
            "Episode 873\tCurrent Score: -83.0\tAverage Score: -99.90748\n",
            "Episode completed in 873 episodes.\n",
            "0:02:12.237661\n",
            "-183.52974828375287\n",
            "4 64\n",
            "Episode 999\tCurrent Score: -121.0\tAverage Score: -201.970:06:41.384486\n",
            "-455.37\n",
            "4 128\n",
            "Episode 999\tCurrent Score: -147.0\tAverage Score: -145.590:05:38.894365\n",
            "-375.831\n",
            "(2, 128)\n"
          ]
        }
      ],
      "source": [
        "least_regret = -int(1e9)\n",
        "for layer, nodes, _, _ in params[1:]:\n",
        "    print(layer, nodes)\n",
        "    model = Reinforce(state_size=state_size, action_size=action_size, seed=-1, num_layers=layer, num_nodes=nodes)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.00001)\n",
        "    critic = Critic(state_size = state_size, action_size=action_size, seed = 0)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=LR)\n",
        "    begin_time = datetime.datetime.now()\n",
        "    scores = reinforce(env, model, optimizer, baseline=False, critic = critic, critic_optimizer = critic_optimizer)\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "    print(time_taken)\n",
        "\n",
        "    if least_regret < sum(scores)/len(scores):\n",
        "        least_regret = sum(scores)/len(scores)\n",
        "        best_params = (layer, nodes)\n",
        "    print(sum(scores)/len(scores))\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize Reinforce 2 for Acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_layers = (2, 3, 4)\n",
        "num_nodes = (64, 128)\n",
        "eps_vals = (0.3, 0.2, 0.1)\n",
        "tau_vals = (0.3, 0.1)\n",
        "# params = [(layers , n, 'eps', e) for layers in num_layers for n in num_nodes for e in eps_vals]\n",
        "params = [(layers , n, 'soft', t) for layers in num_layers for n in num_nodes for t in tau_vals]\n",
        "best_params = None\n",
        "least_regret = -int(1e9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 128\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 945\tCurrent Score: -103.0\tAverage Score: -99.2079\n",
            "Episode completed in 945 episodes.\n",
            "0:02:25.474518\n",
            "-141.3784355179704\n",
            "3 64\n",
            "Episode 999\tCurrent Score: -128.0\tAverage Score: -151.710:04:32.329385\n",
            "-235.475\n",
            "3 128\n",
            "Episode 999\tCurrent Score: -101.0\tAverage Score: -106.950:04:01.381560\n",
            "-204.556\n",
            "4 64\n",
            "Episode 999\tCurrent Score: -144.0\tAverage Score: -167.460:05:50.015319\n",
            "-283.551\n",
            "4 128\n",
            "Episode 999\tCurrent Score: -146.0\tAverage Score: -142.03180:08:04.755695\n",
            "-379.956\n",
            "(2, 128)\n"
          ]
        }
      ],
      "source": [
        "least_regret = -int(1e9)\n",
        "for layer, nodes, _, _ in params[1:]:\n",
        "    model = Reinforce(state_size=state_size, action_size=action_size, seed=-1, num_layers=layer, num_nodes=nodes)\n",
        "    print(layer, nodes)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.00001)\n",
        "    critic = Critic(state_size = state_size, action_size=action_size, seed = -1)\n",
        "    critic_optimizer = optim.SGD(critic.parameters(), lr=0.000001)\n",
        "    begin_time = datetime.datetime.now()\n",
        "    scores = reinforce(env, model, optimizer, baseline=True, critic = critic, critic_optimizer = critic_optimizer)\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "    print(time_taken)\n",
        "    if least_regret > sum(scores)/len(scores):\n",
        "        least_regret = sum(scores)/len(scores)\n",
        "        best_params = (layer, nodes)\n",
        "    print(sum(scores)/len(scores))\n",
        "print(best_params)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CartPole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.action_space.seed(0)\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_layers = (3, 4)\n",
        "num_nodes = (64, 128)\n",
        "eps_vals = (0.3, 0.2, 0.1)\n",
        "# tau_vals = (1000)\n",
        "# params = [(layers , n, 'eps', e) for layers in num_layers for n in num_nodes for e in eps_vals]\n",
        "params = [(layers , n, 'soft', 5000) for layers in num_layers for n in num_nodes]\n",
        "best_params = None\n",
        "least_regret = -int(1e9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize DDkewN type 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 64 soft 5000\n",
            "Episode 12\tCurrent Score: 31.00\tAverage Score: 20.08"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: 22.10\tAverage Score: 22.10\n",
            "Episode 200\tAverage Score: 162.20\tAverage Score: 162.20\n",
            "Episode 222\tCurrent Score: 203.00\tAverage Score: 200.49\n",
            "Environment solved in 222 episodes!\tAverage Score: 200.49\n",
            "0:01:08.906206\n",
            "109.18018018018019\n",
            "3 128 soft 5000\n",
            "Episode 100\tAverage Score: 21.22\tAverage Score: 21.22\n",
            "Episode 200\tAverage Score: 199.48\tAverage Score: 199.48\n",
            "Episode 201\tCurrent Score: 199.00\tAverage Score: 201.24\n",
            "Environment solved in 201 episodes!\tAverage Score: 201.24\n",
            "0:01:03.847081\n",
            "110.7910447761194\n",
            "4 64 soft 5000\n",
            "Episode 100\tAverage Score: 23.67\tAverage Score: 23.67\n",
            "Episode 192\tCurrent Score: 146.00\tAverage Score: 200.05\n",
            "Environment solved in 192 episodes!\tAverage Score: 200.05\n",
            "0:01:10.739965\n",
            "115.6875\n",
            "4 128 soft 5000\n",
            "Episode 100\tAverage Score: 23.89\tAverage Score: 23.89\n",
            "Episode 188\tCurrent Score: 394.00\tAverage Score: 201.67\n",
            "Environment solved in 188 episodes!\tAverage Score: 201.67\n",
            "0:01:11.273700\n",
            "118.54787234042553\n",
            "(4, 128, 'soft', 5000)\n"
          ]
        }
      ],
      "source": [
        "least_regret = -int(1e9)\n",
        "for layer, n, policy, val in params:\n",
        "    print(layer, n, policy, val)\n",
        "    begin_time = datetime.datetime.now()\n",
        "    agent = Agent(state_size=state_size,action_size = action_size,seed = 0, num_layers=layer, num_nodes=n, type=1, lr=0.001)\n",
        "    scores = dqn(env, agent, policy, policy_val=val, thresh=200, decay=0.9999)\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "    if least_regret < sum(scores)/len(scores):\n",
        "        least_regret = sum(scores)/len(scores)\n",
        "        best_params = (layer, n, policy, val)\n",
        "\n",
        "    print(time_taken)\n",
        "    print(sum(scores)/len(scores))\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize DDkewN type 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_layers = (3, 4)\n",
        "num_nodes = (64, 128)\n",
        "eps_vals = (0.3, 0.2, 0.1)\n",
        "# tau_vals = (1000)\n",
        "# params = [(layers , n, 'eps', e) for layers in num_layers for n in num_nodes for e in eps_vals]\n",
        "params = [(layers , n, 'soft', 1) for layers in num_layers for n in num_nodes]\n",
        "best_params = None\n",
        "least_regret = -int(1e9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 64 soft 1\n",
            "Episode 11\tCurrent Score: 34.00\tAverage Score: 22.09"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 81\tCurrent Score: 1000.00\tAverage Score: 208.21\n",
            "Environment solved in 81 episodes!\tAverage Score: 208.21\n",
            "0:00:48.579655\n",
            "208.20987654320987\n",
            "3 128 soft 1\n",
            "Episode 49\tCurrent Score: 1000.00\tAverage Score: 207.76\n",
            "Environment solved in 49 episodes!\tAverage Score: 207.76\n",
            "0:00:29.254604\n",
            "207.75510204081633\n",
            "4 64 soft 1\n",
            "Episode 64\tCurrent Score: 1000.00\tAverage Score: 203.95\n",
            "Environment solved in 64 episodes!\tAverage Score: 203.95\n",
            "0:00:41.967979\n",
            "203.953125\n",
            "4 128 soft 1\n",
            "Episode 71\tCurrent Score: 888.00\tAverage Score: 203.258\n",
            "Environment solved in 71 episodes!\tAverage Score: 203.25\n",
            "0:00:45.593708\n",
            "203.25352112676057\n",
            "(3, 64, 'soft', 1)\n"
          ]
        }
      ],
      "source": [
        "least_regret = -int(1e9)\n",
        "for layer, n, policy, val in params:\n",
        "    print(layer, n, policy, val)\n",
        "    begin_time = datetime.datetime.now()\n",
        "    agent = Agent(state_size=state_size,action_size = action_size,seed = 0, num_layers=layer, num_nodes=n, type=2, lr=0.001)\n",
        "    scores = dqn(env, agent, policy, policy_val=val, thresh=200, decay=0.9999)\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "    if least_regret < sum(scores)/len(scores):\n",
        "        least_regret = sum(scores)/len(scores)\n",
        "        best_params = (layer, n, policy, val)\n",
        "\n",
        "    print(time_taken)\n",
        "    print(sum(scores)/len(scores))\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize Reinforce type 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 64\n",
            "Episode 5\tCurrent Score: 30.0\tAverage Score: 15.50"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 526\tCurrent Score: 275.0\tAverage Score: 200.65\n",
            "Episode completed in 526 episodes.\n",
            "0:00:39.704388\n",
            "121.54459203036053\n",
            "2 128\n",
            "Episode 540\tCurrent Score: 2000.0\tAverage Score: 215.93\n",
            "Episode completed in 540 episodes.\n",
            "0:00:40.660065\n",
            "117.39186691312385\n",
            "3 64\n",
            "Episode 221\tCurrent Score: 108.0\tAverage Score: 200.28\n",
            "Episode completed in 221 episodes.\n",
            "0:00:17.457195\n",
            "107.73423423423424\n",
            "3 128\n",
            "Episode 999\tCurrent Score: 124.0\tAverage Score: 133.110:01:01.674060\n",
            "78.661\n",
            "4 64\n",
            "Episode 341\tCurrent Score: 160.0\tAverage Score: 200.90\n",
            "Episode completed in 341 episodes.\n",
            "0:00:24.563518\n",
            "85.33333333333333\n",
            "4 128\n",
            "Episode 999\tCurrent Score: 9.0\tAverage Score: 9.300:00:23.731076\n",
            "10.68\n",
            "(2, 64)\n"
          ]
        }
      ],
      "source": [
        "least_regret = -int(1e9)\n",
        "for layer, nodes, _, _ in params:\n",
        "    model = Reinforce(state_size=state_size, action_size=action_size, seed=-1, num_layers=layer, num_nodes=nodes)\n",
        "    print(layer, nodes)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    critic = Critic(state_size = state_size, action_size=action_size, seed = -1)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
        "    begin_time = datetime.datetime.now()\n",
        "    scores = reinforce(env, model, optimizer, baseline=False, critic = critic, critic_optimizer = critic_optimizer, thresh = 200)\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "    print(time_taken)\n",
        "    if least_regret < sum(scores)/len(scores):\n",
        "        least_regret = sum(scores)/len(scores)\n",
        "        best_params = (layer, nodes)\n",
        "    print(sum(scores)/len(scores))\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize Reinforce type 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 64\n",
            "Episode 5\tCurrent Score: 19.0\tAverage Score: 13.50"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 499\tCurrent Score: 1258.0\tAverage Score: 202.40\n",
            "Episode completed in 499 episodes.\n",
            "0:01:00.971197\n",
            "118.994\n",
            "2 128\n",
            "Episode 356\tCurrent Score: 454.0\tAverage Score: 200.67\n",
            "Episode completed in 356 episodes.\n",
            "0:00:37.397123\n",
            "93.84873949579831\n",
            "3 64\n",
            "Episode 257\tCurrent Score: 616.0\tAverage Score: 204.710\n",
            "Episode completed in 257 episodes.\n",
            "0:00:29.877406\n",
            "99.76356589147287\n",
            "3 128\n",
            "Episode 989\tCurrent Score: 2000.0\tAverage Score: 202.57\n",
            "Episode completed in 989 episodes.\n",
            "0:02:00.703767\n",
            "112.61313131313132\n",
            "4 64\n",
            "Episode 293\tCurrent Score: 247.0\tAverage Score: 200.43\n",
            "Episode completed in 293 episodes.\n",
            "0:00:33.050459\n",
            "88.35714285714286\n",
            "4 128\n",
            "Episode 999\tCurrent Score: 35.0\tAverage Score: 30.060:01:07.246530\n",
            "41.055\n",
            "(2, 64)\n"
          ]
        }
      ],
      "source": [
        "least_regret = -int(1e9)\n",
        "for layer, nodes, _, _ in params:\n",
        "    model = Reinforce(state_size=state_size, action_size=action_size, seed=-1, num_layers=layer, num_nodes=nodes)\n",
        "    print(layer, nodes)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    critic = Critic(state_size = state_size, action_size=action_size, seed = -1)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=0.0001)\n",
        "    begin_time = datetime.datetime.now()\n",
        "    scores = reinforce(env, model, optimizer, baseline=True, critic = critic, critic_optimizer = critic_optimizer, thresh = 200)\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "    print(time_taken)\n",
        "    if least_regret < sum(scores)/len(scores):\n",
        "        least_regret = sum(scores)/len(scores)\n",
        "        best_params = (layer, nodes)\n",
        "    print(sum(scores)/len(scores))\n",
        "print(best_params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
